# üìö GenAI Research Reading List (Foundations ‚Üí SOTA)

This repository curates **must-read research papers** that define modern Generative AI ‚Äî
from Transformers and Diffusion to LLM reasoning, scaling laws, and agent systems.

Inspired by an industry-curated GenAI reading list and organized for:
- ML / AI Engineers
- Applied Scientists
- Research Engineers
- Interview & system-design prep

---

## üß± Tier 1: Non-Negotiable Pillars
Foundational papers every GenAI practitioner must understand.

- **Attention Is All You Need (Transformers)**  
  https://arxiv.org/pdf/1706.03762.pdf

- **Learning Representations by Back-Propagating Errors (Backpropagation)**  
  https://www.nature.com/articles/323533a0.pdf

- **Language Models Are Few-Shot Learners (GPT-3)**  
  https://arxiv.org/pdf/2005.14165.pdf

- **Denoising Diffusion Probabilistic Models (DDPM)**  
  https://arxiv.org/pdf/2006.11239.pdf

- **CLIP: Learning Transferable Visual Models From Natural Language Supervision**  
  https://arxiv.org/pdf/2103.00020.pdf

---

## üß≠ Tier 2: Enablers
Key breakthroughs that made GenAI practical and scalable.

- **Generative Adversarial Nets (GANs)**  
  https://arxiv.org/pdf/1406.2661.pdf

- **High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion)**  
  https://arxiv.org/pdf/2112.10752.pdf

- **Auto-Encoding Variational Bayes (VAEs)**  
  https://arxiv.org/pdf/1312.6114.pdf

- **ImageNet Classification with Deep Convolutional Neural Networks**  
  https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf

- **Neural Machine Translation by Jointly Learning to Align and Translate (Attention)**  
  https://arxiv.org/pdf/1409.0473.pdf

- **Deep Residual Learning for Image Recognition (ResNet)**  
  https://arxiv.org/pdf/1512.03385.pdf

- **Training Language Models to Follow Instructions with Human Feedback (RLHF / ChatGPT)**  
  https://arxiv.org/pdf/2203.02155.pdf

---

## üöÄ Tier 3: Scaling & SOTA
Modern large-scale and state-of-the-art systems.

- **Video Generation Models as World Simulators**  
  https://arxiv.org/pdf/2405.10989.pdf

- **GPT-4o System Card (Multimodality)**  
  https://cdn.openai.com/gpt-4o-system-card.pdf

- **BERT: Pre-training of Deep Bidirectional Transformers**  
  https://arxiv.org/pdf/1810.04805.pdf

- **Retrieval-Augmented Generation (RAG)**  
  https://arxiv.org/pdf/2005.11401.pdf

- **Scaling Laws for Neural Language Models**  
  https://arxiv.org/pdf/2001.08361.pdf

- **Training Compute-Optimal Large Language Models (Chinchilla)**  
  https://arxiv.org/pdf/2203.15556.pdf

- **Learning to Reason with Large Language Models**  
  https://arxiv.org/pdf/2201.11903.pdf

- **The LLaMA 3 Herd of Models**  
  https://arxiv.org/pdf/2407.21783.pdf

---

## üõ† Tier 4: Practitioner Essentials
What working engineers actually use.

- **QLoRA: Efficient Finetuning of Quantized LLMs**  
  https://arxiv.org/pdf/2305.14314.pdf

- **ReAct: Synergizing Reasoning and Acting in LLMs**  
  https://arxiv.org/pdf/2210.03629.pdf

- **Adam: A Method for Stochastic Optimization**  
  https://arxiv.org/pdf/1412.6980.pdf

- **Mamba: Linear-Time Sequence Modeling**  
  https://arxiv.org/pdf/2312.00752.pdf

- **FlashAttention: Fast and Memory-Efficient Attention**  
  https://arxiv.org/pdf/2205.14135.pdf

- **Mixtral / Sparse Mixture of Experts**  
  https://arxiv.org/pdf/2401.04088.pdf

- **Chain-of-Thought Prompting**  
  https://arxiv.org/pdf/2201.11903.pdf

- **Chatbot Arena: Human-Based Evaluation of LLMs**  
  https://arxiv.org/pdf/2306.05685.pdf

---

## ‚≠ê How to Use
- Read Tier 1 ‚Üí Tier 4 in order
- Focus on **problem ‚Üí idea ‚Üí impact**
- Ideal for interviews, system design, and deep understanding

PRs welcome.
